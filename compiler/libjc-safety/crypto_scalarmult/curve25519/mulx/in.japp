
// h = f + g
// h = 2**0*f0 + 2**64*f1 + 2**128*f2 + 2**192*f3 +
//     2**0*g0 + 2**64*g1 + 2**128*g2 + 2**192*g3

fn _fe64_add_rrs
( reg   u64[4] f,
  stack u64[4] g,
  reg   u64    z
) -> reg u64[4]
{
  inline int i;
  reg bool cf;
  reg u64[4] h;

  h = f;

  cf, h[0] += g[0];
  for i=1 to 4
  { cf, h[i] += g[i] + cf; }

  _, z -= z - cf;
  z &= 38;

  cf, h[0] += z;
  for i=1 to 4
  { cf, h[i] += 0 + cf; }

  _, z -= z - cf;
  z &= 38;
  h[0] += z;

  return h;
}

fn _fe64_add_sss(stack u64[4] fs gs) -> stack u64[4]
{
  stack u64[4] hs;
  reg u64[4] h f;
  reg u64 z;

  z = #set0();
  f = fs;
  h = _fe64_add_rrs(f, gs, z);
  hs = h;

  return hs;
}

fn _fe64_add_ssr(stack u64[4] fs, reg u64[4] g) -> stack u64[4]
{
  stack u64[4] hs;
  reg u64[4] h f;
  reg u64 z;

  z = #set0();
  h = _fe64_add_rrs(g, fs, z);
  hs = h;

  return hs;
}

fn _fe64_add_rsr(stack u64[4] fs, reg u64[4] g) -> reg u64[4]
{
  reg u64[4] h f;
  reg u64 z;

  z = #set0();
  h = _fe64_add_rrs(g, fs, z);

  return h;
}


// h = f - g
// h = (2**0*f0 + 2**64*f1 + 2**128*f2 + 2**192*f3) - 
//     (2**0*g0 + 2**64*g1 + 2**128*g2 + 2**192*g3)

fn _fe64_sub_rrs
( reg   u64[4] f,
  stack u64[4] g,
  reg   u64    z
) -> reg u64[4]
{
  inline int i;
  reg bool cf;
  reg u64[4] h;

  h = f;

  cf, h[0] -= g[0];
  for i=1 to 4
  { cf, h[i] -= g[i] - cf; }

  _, z -= z - cf;
  z &= 38;

  cf, h[0] -= z;
  for i=1 to 4
  { cf, h[i] -= 0 - cf; }

  _, z -= z - cf;
  z &= 38;
  h[0] -= z;

  return h;
}

fn _fe64_sub_sss(stack u64[4] fs gs) -> stack u64[4]
{
  stack u64[4] hs;
  reg u64[4] h f;
  reg u64 z;

  z = #set0();
  f = fs;
  h = _fe64_sub_rrs(f, gs, z);
  hs = h;

  return hs;
}

fn _fe64_sub_rss(stack u64[4] fs gs) -> reg u64[4]
{
  stack u64[4] hs;
  reg u64[4] h f;
  reg u64 z;

  z = #set0();
  f = fs;
  h = _fe64_sub_rrs(f, gs, z);

  return h;
}

fn _fe64_sub_rsr(stack u64[4] fs, reg u64[4] g) -> reg u64[4]
{
  inline int i;
  reg bool cf;
  reg u64[4] h;
  reg u64 z;

  z = #set0();
  h = fs;

  cf, h[0] -= g[0];
  for i=1 to 4
  { cf, h[i] -= g[i] - cf; }

  _, z -= z - cf;
  z &= 38;

  cf, h[0] -= z;
  for i=1 to 4
  { cf, h[i] -= 0 - cf; }

  _, z -= z - cf;
  z &= 38;
  h[0] -= z;

  return h;
}

fn _fe64_sub_ssr(stack u64[4] fs, reg u64[4] g) -> stack u64[4]
{
  stack u64[4] hs;
  reg u64[4] h;

  h = _fe64_sub_rsr(fs, g);
  hs = h;

  return hs;
}


// h = (2**0*f0 + 2**64*f1 + 2**128*f2 + 2**192*f3) * a24
//
// ...
//
// h0  = f0*_a24_l
// h1  = f0*_a24_h
// h1 += f1*_a24_l
// h2  = f1*_a24_h
// h2 += f2*_a24_l
// h3  = f2*_a24_h
// h3 += f3*_a24_l
// h4  = f3*_a24_h


fn _fe64_mul_a24(stack u64[4] f, inline u64 a24) -> reg u64[4]
{
  inline int i;
  reg bool cf;
  reg u64[4] h;
  reg u64 c r0 lo;

  c = a24;

  (h[1], h[0]) = #MULX(c, f[0]);
  (h[2], lo)   = #MULX(c, f[1]);

  cf, h[1] += lo;

  (h[3], lo)   = #MULX(c, f[2]);

  cf, h[2] += lo + cf;

  (r0,   lo)   = #MULX(c, f[3]);

  cf, h[3] += lo + cf;

     _, r0 += 0 + cf;

  _, _, _, _, _, r0 = #IMULri (r0, 38);

  cf, h[0] += r0;
  cf, h[1] += 0 + cf;
  cf, h[2] += 0 + cf;
  cf, h[3] += 0 + cf;

  _, c -= c - cf;

  c &= 38; 
  h[0] += c;

  return h;
}

fn _fe64_mul_a24_ss(stack u64[4] f, inline u64 a24) -> stack u64[4]
{
  stack u64[4] hs;
  reg u64[4] h;

  h = _fe64_mul_a24(f, a24);
  hs = h;

  return hs;
}


// we need to reduce h such that (read comments from x25519_mul.jazz for context):
//
// h =        2**0*h0 + 2**64*h1 + 2**128*h2 + 2**192*h3 +
//     38 * ( 2**0*h4 + 2**64*h5 + 2**128*h6 + 2**192*h7 )
//
// this will yield 5 limbs:
//
// h = 2**0*h0 + 2**64*h1 + 2**128*h2 + 2**192*h3 + 2**256*h4
//
// so h4 should be multiplied by 38 and we should perform an add-carry chain
// - if it propagates a carry until the end: this last bit should be multiplied
//   by 38 and then added to h0 (if the carry happens then, h0 has space left
//   to hold another addition by 38)
//
// in summary : 
//
// h0 += 38*h4_l
// h1 += 38*h4_h
// h1 += 38*h5_l
// h2 += 38*h5_h
// h2 += 38*h6_l
// h3 += 38*h6_h
// h3 += 38*h7_l // if h7 is 2**64-1 then 2**64-1 * 38 will be 0x25_ffff_ffff_ffff_ffda
// h4  = 38*h7_h // the highest limb of 38*h7_h is at most 0x25 which fits in 6 bits
//
// h0 += h4 * 38 // 0x25 is 37 decimal, multiplied by 38, 1406, which fits in 11 bits
//               // worst case scenario: this addition causes 2**256 to be set (by carry
//               // propagation): then h0 is at most 1405 (and has 11 bits at most: thus)
//               // it can handle another addition by 38
// ... 
//
//
// this function returns the input array but only the first 4 limbs should be
// considered

fn _fe64_reduce
( reg u64[4] h,
  reg u64[4] r,
  reg u64 _38,
  reg u64 z, // zero
  reg bool cf of // cf = 0 and of = 0
) -> reg u64[4]
{
  inline int i;
  reg u64 hi lo;

  //
  ( hi, lo )   = #MULX ( _38,  r[0] );
  of, h[0]     = #ADOX ( h[0], lo, of );
  cf, h[1]     = #ADCX ( h[1], hi, cf );

  ( hi, lo )   = #MULX ( _38,  r[1] );
  of, h[1]     = #ADOX ( h[1], lo, of );
  cf, h[2]     = #ADCX ( h[2], hi, cf );

  ( hi, lo )   = #MULX ( _38,  r[2] );
  of, h[2]     = #ADOX ( h[2], lo, of );
  cf, h[3]     = #ADCX ( h[3], hi, cf );

  ( r[0], lo ) = #MULX ( _38, r[3] );
  of, h[3]     = #ADOX ( h[3], lo, of );

  cf, r[0]     = #ADCX ( r[0], z, cf ); 
  of, r[0]     = #ADOX ( r[0], z, of );

  //
  _,_,_,_,_,lo = #IMULri ( r[0], 38 );

  cf, h[0] += lo;
  cf, h[1] += z + cf;
  cf, h[2] += z + cf;
  cf, h[3] += z + cf;

  // h[0] += (z - cf) & 38;
  _, z -= z - cf; // if cf = 1 then z = 0xFF..FF else z = 0
  z &= 38; // if cf = 1 then z = 38 else z = 0
  h[0] += z; // 

  return h;
}


// note: _l stands for the lower 64 bits of multiplication, _h for higher
//
// 
// h = f * g mod 2**255-19
//
// with
//
// f = f0 * 2**0 + f1 * 2**64 + f2 * 2**128 + f3 * 2**192
// g = g0 * 2**0 + g1 * 2**64 + g2 * 2**128 + g3 * 2**192
//
// with each of the limbs being >= 0 and < 2**64
//
// h = f * g
//
// <=>
//
// h = 2**0   * ( f0*g0 )                         +
//     2**64  * ( f0*g1 + f1*g0 )                 +
//     2**128 * ( f0*g2 + f1*g1 + f2*g0 )         +
//     2**192 * ( f0*g3 + f1*g2 + f2*g1 + f3*g0 ) + 
//     2**256 * ( f1*g3 + f2*g2 + f3*g1 )         +
//     2**320 * ( f2*g3 + f3*g2 )                 +
//     2**384 * ( f3*g3 )
//
// <=>
//
// h = 2**0   * ( f0*g0 + ( f1*g3 +   f2*g2 +   f3*g1 ) * 38 ) +
//     2**64  * ( f0*g1 +   f1*g0 + ( f2*g3 +   f3*g2 ) * 38 ) +
//     2**128 * ( f0*g2 +   f1*g1 +   f2*g0 + ( f3*g3 ) * 38 ) +
//     2**192 * ( f0*g3 +   f1*g2 +   f2*g1 +   f3*g0 )
//
// <=>
//
// h = 2**0   * ( f0*g0_l + ( f1*g3_l +   f2*g2_l +   f3*g1_l ) * 38 ) +
//     2**64  * ( f0*g0_h + ( f1*g3_h +   f2*g2_h +   f3*g1_h ) * 38 ) +
//
//     2**64  * ( f0*g1_l +   f1*g0_l + ( f2*g3_l +   f3*g2_l ) * 38 ) +
//     2**128 * ( f0*g1_h +   f1*g0_h + ( f2*g3_h +   f3*g2_h ) * 38 ) +
//
//     2**128 * ( f0*g2_l +   f1*g1_l +   f2*g0_l + ( f3*g3_l ) * 38 ) +
//     2**192 * ( f0*g2_h +   f1*g1_h +   f2*g0_h + ( f3*g3_h ) * 38 ) +
//
//     2**192 * ( f0*g3_l +   f1*g2_l +   f2*g1_l +   f3*g0_l )        +
//     2**256 * ( f0*g3_h +   f1*g2_h +   f2*g1_h +   f3*g0_h )
//
// <=>
//
// h = 2**0   * ( f0*g0_l + ( f1*g3_l +   f2*g2_l +   f3*g1_l ) * 38 ) +
//     2**64  * ( f0*g0_h + ( f1*g3_h +   f2*g2_h +   f3*g1_h ) * 38 ) +
//
//     2**64  * ( f0*g1_l +   f1*g0_l + ( f2*g3_l +   f3*g2_l ) * 38 ) +
//     2**128 * ( f0*g1_h +   f1*g0_h + ( f2*g3_h +   f3*g2_h ) * 38 ) +
//
//     2**128 * ( f0*g2_l +   f1*g1_l +   f2*g0_l + ( f3*g3_l ) * 38 ) +
//     2**192 * ( f0*g2_h +   f1*g1_h +   f2*g0_h + ( f3*g3_h ) * 38 ) +
//
//     2**192 * ( f0*g3_l +   f1*g2_l +   f2*g1_l +   f3*g0_l )
//     2**0   * ( f0*g3_h +   f1*g2_h +   f2*g1_h +   f3*g0_h ) * 38
//
// ...
//
// h = h' + r
//
// where
//
// h' =            2**0*h0 + 2**64*h1 + 2**128*h2 + 2**192*h3
// r  = 38     * ( 2**0*r0 + 2**64*r1 + 2**128*r2 + 2**192*r3 )
//
// but also
//
// r  = 2**256 * ( 2**0*r0 + 2**64*r1 + 2**128*r2 + 2**192*r3 )
//
// now the h0..3/r0..3
//
//   h0 = ( f0*g0_l )
//   h1 = ( f0*g0_h  + f0*g1_l + f1*g0_l )
//   h2 = ( f0*g1_h  + f0*g2_l + f1*g0_h  + f1*g1_l + f2*g0_l )
//   h3 = ( f0*g2_h  + f0*g3_l + f1*g1_h  + f1*g2_l + f2*g0_h  + f2*g1_l + f3*g0_l )
//   r0 = ( f0*g3_h  + f1*g2_h + f1*g3_l  + f2*g1_h + f2*g2_l  + f3*g0_h + f3*g1_l )
//   r1 = ( f1*g3_h  + f2*g2_h + f2*g3_l  + f3*g1_h + f3*g2_l )
//   r2 = ( f2*g3_h  + f3*g2_h + f3*g3_l )
//   r3 = ( f3*g3_h )
//
// and rearranged:
//
//   h0 = ( f0*g0_l )
//   h1 = ( f0*g0_h + f0*g1_l + f1*g0_l )
//   h2 = ( f0*g1_h + f0*g2_l + f1*g0_h + f1*g1_l + f2*g0_l )
//   h3 = ( f0*g2_h + f0*g3_l + f1*g1_h + f1*g2_l + f2*g0_h + f2*g1_l + f3*g0_l )
//   r0 = ( f0*g3_h +           f1*g2_h + f1*g3_l + f2*g1_h + f2*g2_l + f3*g0_h + f3*g1_l )
//   r1 = (                     f1*g3_h +           f2*g2_h + f2*g3_l + f3*g1_h + f3*g2_l )
//   r2 = (                                         f2*g3_h +           f3*g2_h + f3*g3_l )
//   r3 = (                                                             f3*g3_h )
//
// rearranged again (by columns):
//
//   h0  = ( f0*g0_l )
//   h1  = ( f0*g0_h + f0*g1_l )
//   h2  = ( f0*g1_h + f0*g2_l )
//   h3  = ( f0*g2_h + f0*g3_l )
//   r0  = ( f0*g3_h )
//
//   h1 += ( f1*g0_l )
//   h2 += ( f1*g0_h + f1*g1_l )
//   h3 += ( f1*g1_h + f1*g2_l )
//   r0 += ( f1*g2_h + f1*g3_l )
//   r1  = ( f1*g3_h )
//
//   h2 += ( f2*g0_l )
//   h3 += ( f2*g0_h + f2*g1_l)
//   r0 += ( f2*g1_h + f2*g2_l)
//   r1 += ( f2*g2_h + f2*g3_l)
//   r2  = ( f2*g3_h )
//
//   h3 += ( f3*g0_l )
//   r0 += ( f3*g0_h + f3*g1_l )
//   r1 += ( f3*g1_h + f3*g2_l )
//   r2 += ( f3*g2_h + f3*g3_l )
//   r3  = ( f3*g3_h )
//
// more flattening
//
//   h0  = ( f0*g0_l )
//   h1  = ( f0*g0_h ) 
//   h1 += ( f0*g1_l )
//   h2  = ( f0*g1_h )
//   h2 += ( f0*g2_l )
//   h3  = ( f0*g2_h )
//   h3 += ( f0*g3_l )
//   r0  = ( f0*g3_h )
//
//   h1 += ( f1*g0_l )
//   h2 += ( f1*g0_h )
//   h2 += ( f1*g1_l )
//   h3 += ( f1*g1_h )
//   h3 += ( f1*g2_l )
//   r0 += ( f1*g2_h )
//   r0 += ( f1*g3_l )
//   r1  = ( f1*g3_h )
//
//   h2 += ( f2*g0_l )
//   h3 += ( f2*g0_h )
//   h3 += ( f2*g1_l )
//   r0 += ( f2*g1_h )
//   r0 += ( f2*g2_l )
//   r1 += ( f2*g2_h )
//   r1 += ( f2*g3_l )
//   r2  = ( f2*g3_h )
//
//   h3 += ( f3*g0_l )
//   r0 += ( f3*g0_h )
//   r0 += ( f3*g1_l )
//   r1 += ( f3*g1_h )
//   r1 += ( f3*g2_l )
//   r2 += ( f3*g2_h )
//   r2 += ( f3*g3_l )
//   r3  = ( f3*g3_h )
//
// now with carry / overflow flags
//
//   cf, of  = 0, 0
//       h0  = ( f0*g0_l )
//       h1  = ( f0*g0_h ) 
//   cf, h1 += ( f0*g1_l ) + cf
//       h2  = ( f0*g1_h )
//   cf, h2 += ( f0*g2_l ) + cf
//       h3  = ( f0*g2_h )
//   cf, h3 += ( f0*g3_l ) + cf
//       r0  = ( f0*g3_h )
//   cf, r0 += ( 0       ) + cf // f0*g3 <= 2**64-1 * 2**64-1
//                              //          2**128 - 2**65 + 1
//                              // in this case there is at least one bit in r0
//                              // which is zero. so there is no overflow / 
//                              // carry flag : cf = 0
//
//   of, h1 += ( f1*g0_l ) + of
//   cf, h2 += ( f1*g0_h ) + cf
//   of, h2 += ( f1*g1_l ) + of
//   cf, h3 += ( f1*g1_h ) + cf
//   of, h3 += ( f1*g2_l ) + of
//   cf, r0 += ( f1*g2_h ) + cf
//   of, r0 += ( f1*g3_l ) + of
//       r1  = ( f1*g3_h )      
//   cf, r1 += ( 0       ) + cf // CHECK NOTE1
//   of, r1 += ( 0       ) + of // 
//
//////
// NOTE1:
// - worst case scenario (x=1 and y=3 for the last block):
//   fx and gy = 2**64-1
//   so fx*gy = 2**128 - 2**65 + 1
//            = 0xfffffffffffffffe_0000000000000001
//
//   fx*gy_l  = 0x0000000000000001
//   fx*gy_h  = 0xfffffffffffffffe
//
//   TODO
//
//////
// 
//   cf, h2 += ( f2*g0_l ) + cf
//   cf, h3 += ( f2*g0_h ) + cf
//   of, h3 += ( f2*g1_l ) + of
//   cf, r0 += ( f2*g1_h ) + cf
//   of, r0 += ( f2*g2_l ) + of
//   cf, r1 += ( f2*g2_h ) + cf
//   of, r1 += ( f2*g3_l ) + of
//       r2  = ( f2*g3_h )
//   cf, r2 += ( 0       ) + cf // CHECK NOTE1
//   of, r2 += ( 0       ) + of //
//
//   cf, h3 += ( f3*g0_l ) + cf
//   cf, r0 += ( f3*g0_h ) + cf
//   of, r0 += ( f3*g1_l ) + of
//   cf, r1 += ( f3*g1_h ) + cf
//   of, r1 += ( f3*g2_l ) + of
//   cf, r2 += ( f3*g2_h ) + cf
//   of, r2 += ( f3*g3_l ) + of
//       r3  = ( f3*g3_h )
//   cf, r3 += ( 0       ) + cf // CHECK NOTE1
//   of, r3 += ( 0       ) + of //
//
//
// now with temporary variables
//
//   cf, of  = 0, 0
//   h1, h0  = ( f0*g0   )
//
//   h2, t1  = ( f0*g1   )
//   cf, h1 += ( t1      ) + cf
//
//   h3, t2  = ( f0*g2   )
//   cf, h2 += ( t2      ) + cf
//
//   r0, t3  = ( f0*g3   )
//   cf, h3 += ( t3      ) + cf
//
//   cf, r0 += ( 0       ) + cf // cf = 0
//
//////
//
//   t2, t1  = ( f1*g0   )
//   cf, h1 += ( t1      ) + cf
//   cf, h2 += ( t2      ) + cf
//
//   t3, t2  = ( f1*g1   )
//   of, h2 += ( t2      ) + of
//   cf, h3 += ( t3      ) + cf
//
//   t0, t3  = ( f1*g2   )
//   of, h3 += ( t3      ) + of
//   cf, r0 += ( t0      ) + cf
//
//   r1, t0  = ( f1*g3   )
//   of, r0 += ( t0      ) + of
//
//   cf, r1 += ( 0       ) + cf // cf = 0
//   of, r1 += ( 0       ) + of // of = 0
//
//////
//
//   t3, t2  = ( f2*g0   )
//   cf, h2 += ( t2      ) + cf
//   cf, h3 += ( t3      ) + cf
//
//   t0, t3  = ( f2*g1   )
//   of, h3 += ( t3      ) + of
//   cf, r0 += ( t0      ) + cf
//
//   t1, t0  = ( f2*g2   )
//   of, r0 += ( t0      ) + of
//   cf, r1 += ( t1      ) + cf
//
//   r2, t1  = ( f2*g3   )
//   of, r1 += ( t1      ) + of
//
//   cf, r2 += ( 0       ) + cf // cf = 0 (?)
//   of, r2 += ( 0       ) + of // of = 0 (?)
//
//////
//
//   t0, t3  = ( f3*g0   )
//   cf, h3 += ( t3      ) + cf
//   cf, r0 += ( t0      ) + cf
//
//   t1, t0  = ( f3*g1   )
//   of, r0 += ( t0      ) + of
//   cf, r1 += ( t1      ) + cf
//
//   t2, t1  = ( f3*g2   )
//   of, r1 += ( t1      ) + of
//   cf, r2 += ( t2      ) + cf
//
//   r3, t2  = ( f3*g3   )
//   of, r2 += ( t2      ) + of
//
//   cf, r3 += ( 0       ) + cf // cf = 0 (?)
//   of, r3 += ( 0       ) + of // of = 0 (?)
//
// we can split this computation in 2 functions (by merging h and r into a big
// register array called h where r0 is at position 4, and so on...):
// - fe_mul_c0: for the first column
// - fe_mul_cn: for the remaining columns


fn fe64_mul_c0
( reg u64 f0,
  reg u64[4] g,
  reg u64 z, // zero
  reg bool cf of // cf = 0 and of = 0
) -> reg u64[4], reg u64[4], reg bool, reg bool
{
  inline int i;
  reg u64 hi lo;
  reg u64[4] h r;

  (h[1], h[0]) = #MULX ( f0, g[0] );

  ( h[2], lo ) = #MULX ( f0, g[1] );
    cf, h[1]   = #ADCX ( h[1], lo, cf );

  ( h[3], lo ) = #MULX ( f0, g[2] );
    cf, h[2]   = #ADCX ( h[2], lo, cf );

  ( r[0], lo ) = #MULX ( f0, g[3] );
    cf, h[3]   = #ADCX ( h[3], lo, cf );

  cf, r[0] = #ADCX ( r[0], z, cf ); // cf = 0

  return h, r, cf, of;
}

fn fe64_mul_c1
( reg u64[4] h,
  reg u64[4] r,
  reg u64 f,
  reg u64[4] g,
  reg u64 z, // zero
  reg bool cf of // cf = 0 and of = 0
) -> reg u64[4], reg u64[4], reg bool, reg bool
{
  inline int i;
  reg u64 hi lo;

  ( hi, lo )   = #MULX ( f, g[0] );
  of, h[1]     = #ADOX ( h[1], lo, of );
  cf, h[2]     = #ADCX ( h[2], hi, cf );

  ( hi, lo )   = #MULX ( f, g[1] );
  of, h[2]     = #ADOX ( h[2], lo, of );
  cf, h[3]     = #ADCX ( h[3], hi, cf );

  ( hi, lo )   = #MULX ( f, g[2] );
  of, h[3]     = #ADOX ( h[3], lo, of );
  cf, r[0]     = #ADCX ( r[0], hi, cf );

  ( r[1], lo ) = #MULX ( f, g[3] );
  of, r[0]     = #ADOX ( r[0], lo, of);

  cf, r[1]     = #ADCX ( r[1], z, cf);
  of, r[1]     = #ADOX ( r[1], z, of);

  return h, r, cf, of;
}

fn fe64_mul_c2
( reg u64[4] h,
  reg u64[4] r,
  reg u64 f,
  reg u64[4] g,
  reg u64 z, // zero
  reg bool cf of // cf = 0 and of = 0
) -> reg u64[4], reg u64[4], reg bool, reg bool
{
  inline int i;
  reg u64 hi lo;

  ( hi, lo )   = #MULX ( f, g[0] );
  of, h[2]     = #ADOX ( h[2], lo, of );
  cf, h[3]     = #ADCX ( h[3], hi, cf );

  ( hi, lo )   = #MULX ( f, g[1] );
  of, h[3]     = #ADOX ( h[3], lo, of );
  cf, r[0]     = #ADCX ( r[0], hi, cf );

  ( hi, lo )   = #MULX ( f, g[2] );
  of, r[0]     = #ADOX ( r[0], lo, of );
  cf, r[1]     = #ADCX ( r[1], hi, cf );

  ( r[2], lo ) = #MULX ( f, g[3] );
  of, r[1]     = #ADOX ( r[1], lo, of);

  cf, r[2]     = #ADCX ( r[2], z, cf);
  of, r[2]     = #ADOX ( r[2], z, of);

  return h, r, cf, of;
}

fn fe64_mul_c3
( reg u64[4] h,
  reg u64[4] r,
  reg u64 f,
  reg u64[4] g,
  reg u64 z, // zero
  reg bool cf of // cf = 0 and of = 0
) -> reg u64[4], reg u64[4], reg bool, reg bool
{
  inline int i;
  reg u64 hi lo;

  ( hi, lo )   = #MULX ( f, g[0] );
  of, h[3]     = #ADOX ( h[3], lo, of );
  cf, r[0]     = #ADCX ( r[0], hi, cf );

  ( hi, lo )   = #MULX ( f, g[1] );
  of, r[0]     = #ADOX ( r[0], lo, of );
  cf, r[1]     = #ADCX ( r[1], hi, cf );

  ( hi, lo )   = #MULX ( f, g[2] );
  of, r[1]     = #ADOX ( r[1], lo, of );
  cf, r[2]     = #ADCX ( r[2], hi, cf );

  ( r[3], lo ) = #MULX ( f, g[3] );
  of, r[2]     = #ADOX ( r[2], lo, of);

  cf, r[3]     = #ADCX ( r[3], z, cf);
  of, r[3]     = #ADOX ( r[3], z, of);

  return h, r, cf, of;
}


fn _fe64_mul_rsr
( stack u64[4] fs,
  reg u64[4] g
) -> reg u64[4]
{
  reg bool cf of;
  reg u64[4] h r;
  reg u64 _38 f z;

  of, cf, _, _, _, z = #set0();

  f = fs[0];
  h, r, cf, of = fe64_mul_c0(      f, g, z, cf, of);

  f = fs[1];
  h, r, cf, of = fe64_mul_c1(h, r, f, g, z, cf, of);

  f = fs[2];
  h, r, cf, of = fe64_mul_c2(h, r, f, g, z, cf, of);

  f = fs[3];
  h, r, cf, of = fe64_mul_c3(h, r, f, g, z, cf, of);

  _38 = 38;
  h = _fe64_reduce(h, r, _38, z, cf, of);

  return h;
}

fn _fe64_mul_rmr
( reg u64    fs,
  reg u64[4] g
) -> reg u64[4]
{
  reg bool cf of;
  reg u64[4] h r;
  reg u64 _38 f z;

  of, cf, _, _, _, z = #set0();

  f = [fs + 8*0];
  h, r, cf, of = fe64_mul_c0(      f, g, z, cf, of);

  f = [fs + 8*1];
  h, r, cf, of = fe64_mul_c1(h, r, f, g, z, cf, of);

  f = [fs + 8*2];
  h, r, cf, of = fe64_mul_c2(h, r, f, g, z, cf, of);

  f = [fs + 8*3];
  h, r, cf, of = fe64_mul_c3(h, r, f, g, z, cf, of);

  _38 = 38;
  h = _fe64_reduce(h, r, _38, z, cf, of);

  return h;
}


fn _fe64_mul_ssr(stack u64[4] fs, reg u64[4] g) -> stack u64[4]
{
  stack u64[4] hs;
  reg u64[4] h;

  h = _fe64_mul_rsr(fs, g);
  hs = h;

  return hs;
}

fn _fe64_mul_rms(reg u64 fs, stack u64[4] gs) -> reg u64[4]
{
  reg u64[4] h g;

  g = gs;
  h = _fe64_mul_rmr(fs, g);

  return h;
}

fn _fe64_mul_sss(stack u64[4] fs gs) -> stack u64[4]
{
  stack u64[4] hs;
  reg u64[4] h g;

  g = gs;
  h = _fe64_mul_rsr(fs, g);
  hs = h;

  return hs;
}

fn _fe64_mul_rss(stack u64[4] fs gs) -> reg u64[4]
{
  reg u64[4] h g;

  g = gs;
  h = _fe64_mul_rsr(fs, g);

  return h;
}


// note: _l stands for the lower 64 bits of multiplication, _h for higher
//
// 
// h = f * f mod 2**255-19
//
// with
//
// f = f0 * 2**0 + f1 * 2**64 + f2 * 2**128 + f3 * 2**192
//
// with each of the limbs being >= 0 and < 2**64
//
// h = f * g
//
// <=>
//
// h = 2**0   * ( f0*f0 )                         +
//     2**64  * ( f0*f1 + f1*f0 )                 +
//     2**128 * ( f0*f2 + f1*f1 + f2*f0 )         +
//     2**192 * ( f0*f3 + f1*f2 + f2*f1 + f3*f0 ) + 
//     2**256 * ( f1*f3 + f2*f2 + f3*f1 )         +
//     2**320 * ( f2*f3 + f3*f2 )                 +
//     2**384 * ( f3*f3 )
//
// <=>
//
// h = 2**0   * ( f0*f0 + ( f1*f3 +   f2*f2 +   f3*f1 ) * 38 ) +
//     2**64  * ( f0*f1 +   f1*f0 + ( f2*f3 +   f3*f2 ) * 38 ) +
//     2**128 * ( f0*f2 +   f1*f1 +   f2*f0 + ( f3*f3 ) * 38 ) +
//     2**192 * ( f0*f3 +   f1*f2 +   f2*f1 +   f3*f0 )
//
// <=>
//
// h = 2**0   * ( f0*f0_l + ( f1*f3_l +   f2*f2_l +   f3*f1_l ) * 38 ) +
//     2**64  * ( f0*f0_h + ( f1*f3_h +   f2*f2_h +   f3*f1_h ) * 38 ) +
//     2**64  * ( f0*f1_l +   f1*f0_l + ( f2*f3_l +   f3*f2_l ) * 38 ) +
//     2**128 * ( f0*f1_h +   f1*f0_h + ( f2*f3_h +   f3*f2_h ) * 38 ) +
//     2**128 * ( f0*f2_l +   f1*f1_l +   f2*f0_l + ( f3*f3_l ) * 38 ) +
//     2**192 * ( f0*f2_h +   f1*f1_h +   f2*f0_h + ( f3*f3_h ) * 38 ) +
//     2**192 * ( f0*f3_l +   f1*f2_l +   f2*f1_l +   f3*f0_l )        +
//     2**256 * ( f0*f3_h +   f1*f2_h +   f2*f1_h +   f3*f0_h )
//
// <=>
//
// h = 2**0   * ( f0*f0_l + ( f1*f3_l +   f2*f2_l +   f3*f1_l ) * 38 ) +
//     2**64  * ( f0*f0_h + ( f1*f3_h +   f2*f2_h +   f3*f1_h ) * 38 ) +
//     2**64  * ( f0*f1_l +   f1*f0_l + ( f2*f3_l +   f3*f2_l ) * 38 ) +
//     2**128 * ( f0*f1_h +   f1*f0_h + ( f2*f3_h +   f3*f2_h ) * 38 ) +
//     2**128 * ( f0*f2_l +   f1*f1_l +   f2*f0_l + ( f3*f3_l ) * 38 ) +
//     2**192 * ( f0*f2_h +   f1*f1_h +   f2*f0_h + ( f3*f3_h ) * 38 ) +
//     2**192 * ( f0*f3_l +   f1*f2_l +   f2*f1_l +   f3*f0_l )
//     2**0   * ( f0*f3_h +   f1*f2_h +   f2*f1_h +   f3*f0_h ) * 38
//
// ...
//
// h = h' + r
//
// where
//
// h' =            2**0*h0 + 2**64*h1 + 2**128*h2 + 2**192*h3
// r  = 38     * ( 2**0*r0 + 2**64*r1 + 2**128*r2 + 2**192*r3 )
//
// but also
//
// r  = 2**256 * ( 2**0*r0 + 2**64*r1 + 2**128*r2 + 2**192*r3 )
//
// now the h0..3/r0..3
//
//   h0 = ( f0*f0_l )
//   h1 = ( f0*f0_h  + f0*f1_l + f1*f0_l )
//   h2 = ( f0*f1_h  + f0*f2_l + f1*f0_h  + f1*f1_l + f2*f0_l )
//   h3 = ( f0*f2_h  + f0*f3_l + f1*f1_h  + f1*f2_l + f2*f0_h  + f2*f1_l + f3*f0_l )
//   r0 = ( f0*f3_h  + f1*f2_h + f1*f3_l  + f2*f1_h + f2*f2_l  + f3*f0_h + f3*f1_l )
//   r1 = ( f1*f3_h  + f2*f2_h + f2*f3_l  + f3*f1_h + f3*f2_l )
//   r2 = ( f2*f3_h  + f3*f2_h + f3*f3_l )
//   r3 = ( f3*f3_h )
//
// and rearranged:
//
//   h0 = (                                                                  f0*f0_l )
//   h1 = ( ( f0*f1_l)*2                                                   + f0*f0_h )
//   h2 = ( ( f0*f1_h + f0*f2_l)*2                                         + f1*f1_l )
//   h3 = ( (           f0*f2_h + f0*f3_l + f1*f2_l)*2                     + f1*f1_h )
//   r0 = ( (                     f0*f3_h + f1*f2_h + f1*f3_l)*2           + f2*f2_l )
//   r1 = ( (                                         f1*f3_h + f2*f3_l)*2 + f2*f2_h )
//   r2 = ( (                                                   f2*f3_h)*2 + f3*f3_l )
//   r3 = (                                                                  f3*f3_h )
//
// rearranged again (by columns):
//
//   h0  = ( f0*f0_l )
//
//   h1  = ( f0*f1_l )
//   h1 *= 2
//   h1 += ( f0*f0_h )
//
//   h2  = ( f0*f1_h )
//   h2 += ( f0*f2_l )
//   h2 *= 2
//   h2 += ( f1*f1_l )
//
//   h3  = ( f0*f2_h )
//   h3 += ( f0*f3_l )
//   h3 += ( f1*f2_l )
//   h3 *= 2
//   h3 += ( f1*f1_h )
//
//   r0  = ( f0*f3_h )
//   r0 += ( f1*f2_h )
//   r0 += ( f1*f3_l )
//   r0 *= 2
//   r0 += ( f2*f2_l )
//
//   r1  = ( f1*f3_h )
//   r1 += ( f2*f3_l )
//   r1 *= 2
//   r1 += ( f2*f2_h )
//
//   r2  = ( f2*f3_h )
//   r2 *= 2
//   r2 += f3*f3_l
//
//   r3  = ( f3*f3_h )
//
// rearranged again (this can go wrong btw; backtrack until here):
//
//   h0  = ( f0*f0_l )
//   h1  = ( f0*f1_l )
//
//   h2  = ( f0*f1_h )
//   h2 += ( f0*f2_l )
//
//   h3  = ( f0*f2_h ) 
//   h3 += ( f0*f3_l )
//   h3 += ( f1*f2_l )
//
//   r0  = ( f0*f3_h )
//   r0 += ( f1*f2_h )
//   r0 += ( f1*f3_l )
//
//   r1  = ( f1*f3_h )
//   r1 += ( f2*f3_l )
//
//   r2  = ( f2*f3_h )
//                    
//   r3  = ( f3*f3_h )
//
//////
//
//   h1 +=   h1
//   h1 += ( f0*f0_h )
//   h2 +=   h2
//   h2 += ( f1*f1_l )
//   h3 +=   h3
//   h3 += ( f1*f1_h )
//   r0 +=   r0
//   r0 += ( f2*f2_l )
//   r1 +=   r1
//   r1 += ( f2*f2_h )
//   r2 +=   r2
//   r2 +=  f3*f3_l
//   


fn _fe64_sqr_rr(reg u64[4] f) -> reg u64[4]
{
  reg bool cf of;
  inline int i;
  reg u64[8] t;
  reg u64[4] h r;
  reg u64 z _38 fx;

  of, cf, _, _, _, z = #set0();

  // 0
  fx = f[0];

  (t[1], h[0]) = #MULX ( fx,   fx       ); // f0*f0
  (h[2], h[1]) = #MULX ( fx,   f[1]     ); // f0*f1

  (h[3], t[2]) = #MULX ( fx,   f[2]     ); // f0*f2
     cf, h[2]  = #ADCX ( h[2], t[2], cf );

  (r[0], t[3]) = #MULX ( fx,   f[3]     ); // f0*f3
     cf, h[3]  = #ADCX ( h[3], t[3], cf );

  // 1
  fx = f[1];

  (t[4], t[3]) = #MULX ( fx,   f[2]     ); // f1*f2

  of, h[3]     = #ADOX ( h[3], t[3], of );
  cf, r[0]     = #ADCX ( r[0], t[4], cf );
  
  (r[1], t[4]) = #MULX ( fx,   f[3]     ); // f1*f3
     of, r[0]  = #ADOX ( r[0], t[4], of );

  (t[3], t[2]) = #MULX ( fx,   fx       ); // f1*f1

  // 2
  fx = f[2];

  (r[2], t[5]) = #MULX ( fx,   f[3]     ); // f2*f3

   cf, r[1]    = #ADCX ( r[1], t[5], cf );
   of, r[1]    = #ADOX ( r[1], z,    of );

   cf, r[2]    = #ADCX ( r[2], z,    cf ); // cf = 0
   of, r[2]    = #ADOX ( r[2], z,    of ); // of = 0 ?? TODO: VERIFYME

  (t[5], t[4]) = #MULX ( fx,   fx       ); // f2*f2

  // 3
  fx = f[3];

  (r[3], t[6]) = #MULX ( fx,   fx       ); // f3*f3

  //
  cf, h[1] = #ADCX ( h[1], h[1], cf );
  of, h[1] = #ADOX ( h[1], t[1], of );

  cf, h[2] = #ADCX ( h[2], h[2], cf );
  of, h[2] = #ADOX ( h[2], t[2], of );

  cf, h[3] = #ADCX ( h[3], h[3], cf );
  of, h[3] = #ADOX ( h[3], t[3], of );

  cf, r[0] = #ADCX ( r[0], r[0], cf );
  of, r[0] = #ADOX ( r[0], t[4], of );

  cf, r[1] = #ADCX ( r[1], r[1], cf );
  of, r[1] = #ADOX ( r[1], t[5], of );

  cf, r[2] = #ADCX ( r[2], r[2], cf );
  of, r[2] = #ADOX ( r[2], t[6], of );

  cf, r[3] = #ADCX ( r[3], z,    cf ); // cf = 0
  of, r[3] = #ADOX ( r[3], z,    of ); // of = 0 ?? TODO: VERIFYME

  _38 = 38;
  h = _fe64_reduce(h, r, _38, z, cf, of);

  return h;
}

fn _fe64_it_sqr(stack u64 i, reg u64[4] f) -> stack u64, reg u64[4]
{
  reg bool zf;
  reg u64[4] h;

  while
  {
    h = _fe64_sqr_rr(f);
    (_, _, _, _, i) = #DEC(i);

    f = _fe64_sqr_rr(h);
    (_, _, _, zf, i) = #DEC(i);

  } (!zf)

  return i, f;
}

fn _fe64_sqr_ss(stack u64[4] fs) -> stack u64[4]
{
  stack u64[4] hs;
  reg u64[4] f h;

  f = fs;
  h = _fe64_sqr_rr(f);
  hs = h;

  return hs;
}

fn _fe64_sqr_sr(reg u64[4] f) -> stack u64[4]
{
  stack u64[4] hs;
  reg u64[4] h;

  h = _fe64_sqr_rr(f);
  hs = h;

  return hs;
}

fn _fe64_sqr_rs(stack u64[4] fs) -> reg u64[4]
{
  reg u64[4] f h;

  f = fs;
  h = _fe64_sqr_rr(f);

  return h;
}


// supercop * / crypto_scalarmult / curve25519 ref10 implementation
fn _fe64_invert(reg u64[4] f) -> reg u64[4]
{
  stack u64 i;
  stack u64[4] fs t0s t1s t2s t3s;
  reg u64[4] t0  t1  t2  t3;

  fs = f;

  // z2 = z1^2^1
  t0  = _fe64_sqr_rr(f);
  t0s = t0;

  // z8 = z2^2^2
  t1  = _fe64_sqr_rr(t0);
  t1  = _fe64_sqr_rr(t1);

  // z9 = z1*z8
  t1  = _fe64_mul_rsr(fs,t1);
  t1s = t1;

  // z11 = z2*z9
  t0  = _fe64_mul_rsr(t0s,t1);
  t0s = t0;

  // z22 = z11^2^1
  t2 = _fe64_sqr_rr(t0);

  // z_5_0 = z9*z22
  t1  = _fe64_mul_rsr(t1s,t2);
  t1s = t1;

  // z_10_5 = z_5_0^2^5
  t2 = _fe64_sqr_rr(t1);
  i = 4; i, t2 = _fe64_it_sqr(i, t2); 
  t2s = t2;

  // z_10_0 = z_10_5*z_5_0
  t1  = _fe64_mul_rsr(t1s,t2);
  t1s = t1;

  // z_20_10 = z_10_0^2^10
  i = 10; i, t2 = _fe64_it_sqr(i, t1);

  // z_20_0 = z_20_10*z_10_0
  t2  = _fe64_mul_rsr(t1s,t2);
  t2s = t2;

  // z_40_20 = z_20_0^2^20
  i = 20; i, t3 = _fe64_it_sqr(i, t2);

  // z_40_0 = z_40_20*z_20_0
  t2  = _fe64_mul_rsr(t2s,t3);

  // z_50_10 = z_40_0^2^10
  i = 10; i, t2 = _fe64_it_sqr(i, t2);

  // z_50_0 = z_50_10*z_10_0
  t1  = _fe64_mul_rsr(t1s,t2);
  t1s = t1;

  // z_100_50 = z_50_0^2^50
  i = 50; i, t2 = _fe64_it_sqr(i, t1);

  // z_100_0 = z_100_50*z_50_0
  t2  = _fe64_mul_rsr(t1s,t2);
  t2s = t2;

  // z_200_100 = z_100_0^2^100
  i = 100; i, t3 = _fe64_it_sqr(i, t2);

  // z_200_0 = z_200_100*z_100_0
  t2  = _fe64_mul_rsr(t2s,t3);

  // z_250_50 = z_200_0^2^50
  i = 50; i, t2 = _fe64_it_sqr(i, t2);

  // z_250_0 = z_250_50*z_50_0
  t1  = _fe64_mul_rsr(t1s,t2);

  // z_255_5 = z_250_0^2^5
  i = 4; i, t1 = _fe64_it_sqr(i, t1);
  t1 = _fe64_sqr_rr(t1);

  // z_255_21 = z_255_5*z11
  t1 = _fe64_mul_rsr(t0s,t1);

  return t1;
}


// implements openssl strategy : clever
fn _fe64_tobytes(reg u64[4] f) -> reg u64[4]
{
  reg bool cf;
  reg u64 t;

  t = f[3] + f[3];
  f[3] = #SAR(f[3], 63);
  t >>= 1;
  f[3] &= 19;
  f[3] += 19;

  cf, f[0] += f[3];
  cf, f[1] += 0 + cf;
  cf, f[2] += 0 + cf;
  cf, t    += 0 + cf;

  f[3] = t + t;
  t    = #SAR(t, 63);
  f[3] >>= 1;
  t = !t;
  t &= 19;

  cf, f[0] -= t;
  cf, f[1] -= 0 - cf;
  cf, f[2] -= 0 - cf;
  cf, f[3] -= 0 - cf;

  return f;
}


fn _fe64_cswap(stack u64[4] x2,
               reg   u64[4] z2r,
               stack u64[4] x3,
               stack u64[4] z3,
               reg   u64    toswap) -> stack u64[4],
                                       reg   u64[4],
                                       stack u64[4],
                                       stack u64[4]
{
  inline int i;
  reg u64[4] t4 x2r, x3r, z3r;
  reg u64 t mask;

  mask = #set0();
  mask -= toswap; // if toswap == 1 mask = -1 or all bits at 1, 0 otherwise

  // swap between z2r and z3
  z3r = z3;
  t4  = z2r;

  for i=0 to 4 { t4[i]  ^= z3r[i]; }
  for i=0 to 4 { t4[i]  &= mask;   }
  for i=0 to 4 { z2r[i] ^= t4[i];
                 z3r[i] ^= t4[i];
                 z3[i]   = z3r[i]; }

  // swap between z2r and z3
  x3r = x3;

  for i=0 to 4 { x2r[i]  = x2[i];
                 t       = x3r[i];
                 t      ^= x2r[i];
                 t      &= mask;
                 x2r[i] ^= t;
                 x3r[i] ^= t;
                 x2[i]   = x2r[i];
                 x3[i]   = x3r[i]; }

  return x2, z2r, x3, z3;
}

fn _fe64_cswap_ssss(
  stack u64[4] xs,
  stack u64[4] ys,
  reg u64 swap
) -> stack u64[4], stack u64[4]
{
  inline int i;
  reg u64[4] x y;
  reg u64 t mask;

  x = xs;

  mask = 0;
  mask -= swap;

  for i=0 to 4
  {
    y[i] = ys[i];

    t  = x[i];
    t ^= y[i];
    t &= mask;

    x[i] ^= t; // ^ (x[i] ^ y[i]) if swap == 1 
    y[i] ^= t;

    ys[i] = y[i];
  }

  xs = x;

  return xs, ys;
}

fn _fe64_cswap_rsrs(
  reg   u64[4] x,
  stack u64[4] ys,
  reg u64 swap
) -> reg u64[4], stack u64[4]
{
  inline int i;
  reg u64[4] y;
  reg u64 t mask;

  mask = 0;
  mask -= swap;

  for i=0 to 4
  {
    y[i] = ys[i];

    t  = x[i];
    t ^= y[i];
    t &= mask;

    x[i] ^= t; // ^ (x[i] ^ y[i]) if swap == 1 
    y[i] ^= t;

    ys[i] = y[i];
  }

  return x, ys;
}


fn ith_bit(stack u8[32] k, reg u64 ctr) -> reg u64
{
  reg u64 p bit;

  p = ctr;
  p >>= 3;
  bit = (64u) k[(int) p];

  p = ctr;
  p &= 7;
  bit >>= p;

  bit &= 1;

  return bit;
}

fn decode_scalar_25519(reg u64 kp) -> stack u8[32]
{
  inline int i;
  stack u8[32] k;
  reg u64 t;

  for i=0 to 4
  { t = [kp + 8*i];
    k[u64 i] = t; }

  k[0]  &= 0xf8;
  k[31] &= 0x7f;
  k[31] |= 0x40;

  return k;
}

fn decode_u_coordinate(reg u64 up) -> reg u64[4]
{
  inline int i;
  reg u64[4] u;

  for i=0 to 4
  { u[i] = [up + 8*i]; }
  u[3] &= 0x7fffffffffffffff;

  return u;
}

fn init_points(reg u64[4] initr) -> stack u64[4], reg u64[4], stack u64[4], stack u64[4]
{
  inline int i;
  stack u64[4] x2 x3 z3;
  reg u64[4] z2r;
  reg u64 z;

  z = #set0();

  x2[0] = 1;
  z2r[0] = 0;
  x3 = initr;
  z3[0] = 1;

  for i=1 to 4
  { x2[i] = z;
    z2r[i] = z;
    z3[i] = z; }

  //     (1,   0, init, 1)
  return x2, z2r, x3,  z3;
}

fn add_and_double(stack u64[4] init,
                  stack u64[4] x2,
                  reg   u64[4] z2r,
                  stack u64[4] x3,
                  stack u64[4] z3) -> stack u64[4],
                                      reg   u64[4],
                                      stack u64[4],
                                      stack u64[4]
{
  stack u64[4] z2 t0 t1 t2;
  reg u64[4] t1r;

  t0  = _fe64_sub_ssr(x2, z2r);
  x2  = _fe64_add_ssr(x2, z2r);

  t1  = _fe64_sub_sss(x3, z3);
  z2  = _fe64_add_sss(x3, z3);

  z3  = _fe64_mul_sss(x2, t1);
  z2  = _fe64_mul_sss(z2, t0);

  t2  = _fe64_sqr_ss(x2);
  t1r = _fe64_sqr_rs(t0);

  x3  = _fe64_add_sss(z3, z2);
  z2  = _fe64_sub_sss(z3, z2);

  x2  = _fe64_mul_ssr(t2, t1r);
  t0  = _fe64_sub_ssr(t2, t1r);

  z2  = _fe64_sqr_ss(z2);
  z3  = _fe64_mul_a24_ss(t0, 121665);
  x3  = _fe64_sqr_ss(x3);

  t2  = _fe64_add_sss(t2, z3);
  z3  = _fe64_mul_sss(init, z2);
  z2r = _fe64_mul_rss(t0, t2);

  return x2, z2r, x3, z3;
}

fn montgomery_ladder_step(stack u8[32] k,
                          stack u64[4] init,
                          stack u64[4] x2,
                          reg   u64[4] z2r,
                          stack u64[4] x3,
                          stack u64[4] z3,
                          stack u64    swapped,
                          reg   u64    ctr) -> stack u64[4],
                                               reg   u64[4],
                                               stack u64[4],
                                               stack u64[4],
                                               stack u64
{
  reg u64 toswap bit;

  bit = ith_bit(k, ctr);

  toswap  = swapped;
  toswap ^= bit;

  x2, z2r, x3, z3 = _fe64_cswap(x2, z2r, x3, z3, toswap);
  swapped = bit;

  x2, z2r, x3, z3 = add_and_double(init, x2, z2r, x3, z3);

  return x2, z2r, x3, z3, swapped;
}


fn montgomery_ladder(reg u64[4] initr, stack u8[32] k) -> stack u64[4],
                                                          reg u64[4],
                                                          stack u64[4],
                                                          stack u64[4]
{
  stack u64[4] init x2 x3 z3;
  reg u64[4] z2r;
  stack u64 ctrs swapped;
  reg u64 ctr bit;

  (x2,z2r,x3,z3) = init_points(initr); 
  init = initr;

  ctr = 255;
  swapped = 0;

  while
  {
    ctr -= 1;
    ctrs = ctr;

    (x2, z2r, x3, z3, swapped) = montgomery_ladder_step(k, init, x2, z2r, x3, z3, swapped, ctr);

    ctr = ctrs;
  } (ctr > 0)

  return x2, z2r, x3, z3;
}

fn encode_point(stack u64[4] x2, reg u64[4] z2r) -> reg u64[4]
{
  reg u64[4] r;

  z2r = _fe64_invert(z2r);
  r = _fe64_mul_rsr(x2, z2r);
  r = _fe64_tobytes(r);

  return r;
}


fn _x25519_scalarmult(
  reg u64 rp,
  reg u64 kp,
  reg u64 up
)
{
  inline int i;
  stack u8[32] k;
  stack u64[4] x2 x3 z3;
  reg u64[4] u z2r r;
  reg u64 swap pos b;
  stack u64 rps swaps poss;

  rps = rp; // rp dead

  k = decode_scalar_25519(kp); // kp dead
  u = decode_u_coordinate(up); // up dead
  (x2,z2r,x3,z3) = montgomery_ladder(u, k);
  r = encode_point(x2,z2r);

  rp = rps;
  for i=0 to 4
  { [rp + 8*i] = r[i]; }
}



//////

fn _fe64_1_x3() -> stack u64[4], reg u64[4], stack u64[4]
{
  inline int i;
  stack u64[4] f1s f3s;
  reg   u64[4] f2;
  reg   u64 z;

  z = #set0();

  f1s[0] = 1;
  f2[0]  = 1;
  f3s[0] = 1;

  for i=1 to 4
  { f1s[i] = z;
    f2[i]  = z;
    f3s[i] = z;
  }

  return f1s, f2, f3s;
}

// "How to (pre-)compute a ladder"
// implementation in C of the paper: https://github.com/armfazh/rfc7748_precomputed
fn _x25519_scalarmult_base(
  reg u64 out,
  reg u64 scalar,
  reg u64 table
)
{
  inline int i;
  stack u64[4] u1 z1 u2 z2 t1 t2 t3 t4;
  reg   u64[4] z1r u2r t1r t2r t3r;
  stack u8[32] e;
  reg u64 t swap pos b;
  stack u64 outs tables swaps poss;

  outs = out; // out dead
  tables = table; // table dead

  for i=0 to 4
  { t = [scalar + 8*i];
    e[u64 i] = t; } // scalar dead

  e[0]  &= 0xf8;
  e[31] &= 0x7f;
  e[31] |= 0x40;

  u1, z1r, z2 = _fe64_1_x3();

	u2r[0] = 0x7e94e1fec82faabd;
	u2r[1] = 0xbbf095ae14b2edf8;
	u2r[2] = 0xadc7a0b9235d48e2;
	u2r[3] = 0x1eaecdeee27cab34;

  u2 = u2r;

  pos = 3;
  swaps = 1;
  while
  {
    poss = pos;
    swap = swaps;

    b = ith_bit(e, pos);

    swap ^= b;

    u1,  u2 = _fe64_cswap_ssss(u1,  u2, swap);
    z1r, z2 = _fe64_cswap_rsrs(z1r, z2, swap);

    swaps = b;

    t2    = _fe64_sub_ssr(u1, z1r);
    t1    = _fe64_add_ssr(u1, z1r);

    table = tables;
    t3r   = _fe64_mul_rms(table, t2);

    t2    = _fe64_sub_ssr(t1, t3r);
    t1r   = _fe64_add_rsr(t1, t3r);

    t1    = _fe64_sqr_sr(t1r);
    t2    = _fe64_sqr_ss(t2);

    u1    = _fe64_mul_sss(z2, t1);
    z1r   = _fe64_mul_sss(u2, t2);

    tables += 8*4;

    pos = poss;
    pos += 1;
  } (pos < 255)

  pos = 0;
  while
  {
    poss = pos;

    t1 = _fe64_add_ssr(u1, z1r);
    t2 = _fe64_sub_ssr(u1, z1r);

    t1  = _fe64_sqr_ss(t1);
    t2r = _fe64_sqr_rs(t2);

    t3  = t2r;

    t2 = _fe64_sub_ssr(t1, t2r);

    //t4 = _fe64_mul121666_ss(t2);
    t4 = _fe64_mul_a24_ss(t2, 121666);

    t4 = _fe64_add_sss(t4, t3);

    u1  = _fe64_mul_sss(t1, t3);
    z1r = _fe64_mul_rss(t2, t4);

    pos = poss;
    pos += 1;
  } (pos < 3)

  t1r = _fe64_invert(z1r);
  t1r = _fe64_mul_rsr(u1, t1r);
  t1r = _fe64_tobytes(t1r);

  out = outs;
  for i=0 to 4
  { [out + 8*i] = t1r[i]; }
}

export fn curve25519_mulx(reg u64 out scalar point)
{
  _x25519_scalarmult(out, scalar, point);
}

export fn curve25519_mulx_base(reg u64 out scalar table)
{
  _x25519_scalarmult_base(out, scalar, table);
}
